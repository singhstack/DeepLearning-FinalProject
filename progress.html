<!doctype html>
<html lang="en">

<head>
  <title>Improving NIC</title>
  <meta property="og:title" content=Your Project Name" />
  <meta name="twitter:title" content="Your Project Name" />
  <meta name="description" content="Your project about your cool topic described right here." />
  <meta property="og:description" content="Your project about your cool topic described right here." />
  <meta name="twitter:description" content="Your project about your cool topic described right here." />
  <meta property="og:type" content="website" />
  <meta name="twitter:card" content="summary" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <!-- bootstrap for mobile-friendly layout -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css"
    integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js"
    integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct"
    crossorigin="anonymous"></script>
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
  <link href="style.css" rel="stylesheet">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


</head>

<body class="nd-docs">
  <div class="nd-pageheader">
    <div class="container">
      <h1 class="lead">
        <nobr class="widenobr">Deep Learning Final Project</nobr>
        <nobr class="widenobr">For CS 7150</nobr>
      </h1>
    </div>
  </div><!-- end nd-pageheader -->

  <div class="container">
    <div class="row">
      <div class="col justify-content-center text-center">
        <h2>Augmentation and Adjustment: Can we improve Neural Image Caption model performance?</h2>
        <p>Project Progress: Week 2</p>
      </div>
    </div>
    <div class="row">
      <div class="col">

        <h2>Dataset and Block Diagram</h2>
        
        <p>Here are some sample images along with the annotations from the COCO Dataset that we have used for our project:</p>
        <table style="width: 100%;margin-bottom: 30px;">
          <tr>
            <td><img style="width: 100%;height: 100%;" alt="" src="img/image4.png"></img></td>
            <td><img style="width: 100%;height: 100%;" alt="" src="img/image5.png"></img></td>
            <td><img style="width: 100%;height: 100%;" alt="" src="img/image6.png"></img></td>
            <td><img style="width: 100%;height: 100%;" alt="" src="img/image7.png"></img></td>
          </tr>
        <tr>  </tr>
        </table>

        <p>The diagram below presents a high level description of our implementation:</p>
        <table style="width: 50%;margin: auto;">
          <tr>
            <td><img style="width: 100%;height: 100%; margin: auto;" alt="" src="img/image10.png"></img></td>
          </tr>
        </table>
        <h2>A Conceptual Review</h2>
        <p>
          The original Neural Image Caption (NIC) model, as described in the 2015 paper by Oriol Vinyals, et. al.,
          is composed of a deep convolutional neural network (CNN) and a recurrent neural network (RNN), which act
          as the encoder and decoder, respectively. The CNN was first pre-trained for image classification 
          tasks, then used to embed the input image into a fixed-length vector. The last hidden layer is thereafter provided 
          as visual input directly to the RNN. The authors chose to use a Long-Short Term Memory (LSTM) net as the RNN for
          sequence modeling because of its ability to handle vanishing and exploding gradients. The loss function is the sum of the 
          negative log likelihood of the correct word at every step: 
          <br>
          <br>
          
          \(L(I, S)=-\sum_{t=1}^N \log p_t\left(S_t\right)\). 
          <br>
          <br>
          Where I is the image,  S is the sentence, and t is the step. For inference and sentence generation, the BeamSearch 
          approach was selected, which iteratively considers the set of k best sentences up to time t.


        </p>

      

        <h2>Implementation Details</h2>

        <p> 
          For our dataset, we used <a href='[2]'>COCOAPI [2]</a> for annotations and images. For hyperparameters, we used 128 batch size, 
          300 embedding size, 3 epochs, 0.001 learning rate, and total steps is length of caption size divided by batch size.

 
        </p>

        <h2>Findings</h1>

          <p>
            At first we attempted to use <a href='#[4]'> Tong’s model [4]</a> for NIC, but had issues downloading Visual Genome from Stanford. 
            Eventually, we found a model created by <a href='#[4]'> Shwetank [3]</a> that allowed us to train a model with a publicly available dataset. 
            However, there were many difficulties loading the actual dataset into the original model due to compiling complications. 
            The model ran as expected and we were able to capture text from images.
            <br>
            <br>
            Before running the model, we expected that the process to load/download data and run an existing model. However, Tong’s model 
            needed the bottom up attention approach to extract annotations from images. Surprisingly, the method labels segments of each 
            image and records the areas of these segments. Shwetank’s approach used COCOAPI that already had annotations extracted from each image.
            <br>
            <br>
            A major obstacle that we avoided was to not recode something that exists as an online resource. We also decided to utilize 
            the model’s pre-existing dataset. 

          </p>

        <h2> References</h2>
        <p><a name="[1]">[1]</a> <a href="https://papers.baulab.info/Vinyals-2015.pdf">
          Vinyals, Oriol, et al. <em>Show and tell: A neural image caption generator.</em> </a> Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.</p>
        <p><a name="[2]">[2]</a> <a href="https://github.com/cocodataset/cocoapi">
          https://github.com/cocodataset/cocoapi</a> </p>
        <p><a name="[3]">[3]</a> <a href="https://github.com/pshwetank/NIC-2015-Pytorch">
          https://github.com/pshwetank/NIC-2015-Pytorch </a> </p>
        <p><a name="[4]">[4]<a href="https://github.com/ezeli/NIC_model">
          https://github.com/ezeli/NIC_model </a> </p>
          
        <h2>Team Members</h2>
        <ul>
          <li><a href = https://www.linkedin.com/in/mary-dao-4babb2215>Mary Dao</a></li>
          <li><a href = https://www.linkedin.com/in/kevin-russell-3b849087>Kevin Russell</a></li>
          <li><a href = https://www.linkedin.com/in/tarandeep-singh-947135104>Tarandeep Singh</a></li>
        </ul>

      </div>
      <!--col-->
    </div>
    <!--row -->
  </div> <!-- container -->

  <footer class="nd-pagefooter">
    <div class="row">
      <div class="col-6 col-md text-center">
        <a href="https://cs7150.baulab.info/">About CS 7150</a>
      </div>
    </div>
  </footer>

</body>
<script>
  $(document).on('click', '.clickselect', function (ev) {
    var range = document.createRange();
    range.selectNodeContents(this);
    var sel = window.getSelection();
    sel.removeAllRanges();
    sel.addRange(range);
  });
  // Google analytics below.
  window.dataLayer = window.dataLayer || [];
</script>

</html>